{
  "metadata": {
    "kernelspec": {
      "name": "xpython",
      "display_name": "Python 3.13 (XPython)",
      "language": "python"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "a5785128-8c58-4ef4-ba44-8ea48d51d151",
      "cell_type": "code",
      "source": "**Data Preparation & Understanding**\n\n○      Summarize dataset and attributes.\n\n○      Handle missing values, perform preprocessing/feature engineering as needed.\n\n○      Provide exploratory analysis with tables/visualizations.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'SyntaxError'>",
          "evalue": "invalid character '○' (U+25CB) (3141712825.py, line 3)",
          "traceback": [
            "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m○      Summarize dataset and attributes.\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid character '○' (U+25CB)\n"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 2
    },
    {
      "id": "6c84067c-0b5f-4e57-acac-16236ada7e1f",
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\n# Upload the file in Jupyter first: Files → Upload\ndf=pd.read_csv(\"bank.csv\")\ndisplay(df.head())\ndf.info()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "31e2e985-0eb0-4f67-84cf-026fc0fe311f",
      "cell_type": "code",
      "source": "# 1) Print only the number of rows.\n# 2) Show the first 10 rows.\n#  df.shape[0], df.head(10)\n\ndf.shape[0]\ndf.head(10)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6c467dfb-e33e-4b49-b068-b7a93d78e4fa",
      "cell_type": "code",
      "source": "# select by position (iloc) and by name (loc)\ndf.iloc[0:5, 0:4]          # rows 0–4, columns 0–3\ndf.loc[0:4, [\"age\", \"balance\", \"duration\"]]  # rows 0–4, named columns\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b6096a29-889b-4970-b8c4-a1be7444eb99",
      "cell_type": "code",
      "source": "# Create a NEW feature using existing columns from bank.csv\n# Example: Product of 'age' and 'balance'\ndf.loc[:, \"age_balance_product\"] = df[\"age\"] * df[\"balance\"]\ndf[[\"age\", \"balance\", \"age_balance_product\"]].head()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "97d5e6a9-2777-4d7c-a1d7-1ea6665df838",
      "cell_type": "code",
      "source": "#  Using .loc, create a new feature using existing numerical columns\n# Example: balance_per_age = balance / age\n\ndf.loc[:, \"balance_per_age\"] = df[\"balance\"] / df[\"age\"]\n\n# Replace infinite values in the newly created column if any\ndf['balance_per_age'] = df['balance_per_age'].replace([float('inf'), float('-inf')], np.nan)\ndf['balance_per_age'].fillna(df['balance_per_age'].median(), inplace=True)\n\n# Display first 5 rows and selected columns to show the new feature\ndf[['age', 'balance', 'balance_per_age']].head()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8c1cf4d6-bf0b-4415-a625-c7aa0bc0181e",
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\n\nbank = pd.read_csv('bank.csv')\nprint('Poutcome:', bank.poutcome)\nbank.info()\n\n# check missing values\nbank.isnull().sum()\n# Fill numeric columns with mean (safe even if none are missing)\nbank.fillna(bank.mean(numeric_only=True), inplace=True)\nprint('Total missing after fill:', bank.isnull().sum().sum())",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "add70d99-10bd-4551-ba8a-f264edb609a4",
      "cell_type": "code",
      "source": "from sklearn.preprocessing import StandardScaler\nimport numpy as np # Ensure numpy is imported for select_dtypes\n\nX = df.drop('deposit', axis=1)\n\n# Select only numerical columns for StandardScaler\nnumerical_cols = X.select_dtypes(include=np.number).columns\nX_numeric = X[numerical_cols]\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_numeric)\nprint('Scaled shape:', X_scaled.shape)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "fff9e543-dfb6-4d87-8eff-a7a1643b05d1",
      "cell_type": "code",
      "source": "# define X and y here\n\n# Drop any created feature you do not want to include (choose to keep either px_area or px_aspect, or both—your call).\nX = df.drop(columns=[\"deposit\"])\ny = df[\"deposit\"]\n\n# Identify numerical columns for operations\nnumerical_cols = X.select_dtypes(include=np.number).columns\n\n# Replace infinite values in numerical columns of X with the median of each column\nfor col in numerical_cols:\n    # Using .loc to avoid FutureWaring with inplace=True on a copy\n    X.loc[:, col] = X[col].replace([float('inf'), float('-inf')], X[col].median())\n    # Fill NaN values in numerical columns with the median\n    X.loc[:, col] = X[col].fillna(X[col].median())",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "93d42c67-9b9f-465b-aff4-78da38a8d503",
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# Load the dataset (as in cell e7ba806a)\ndf = pd.read_csv(\"bank.csv\")\n\n# Feature engineering: age_balance_product (as in cell qprRfdTKPC-t)\ndf.loc[:, \"age_balance_product\"] = df[\"age\"] * df[\"balance\"]\n\n# Feature engineering: balance_per_age (as in cell MEdgNfIcPHcq)\ndf.loc[:, \"balance_per_age\"] = df[\"balance\"] / df[\"age\"]\ndf['balance_per_age'] = df['balance_per_age'].replace([float('inf'), float('-inf')], np.nan)\ndf['balance_per_age'] = df['balance_per_age'].fillna(df['balance_per_age'].median()) # Modified line to avoid FutureWarning\n\n# Drop any created feature you do not want to include (choose to keep either px_area or px_aspect, or both—your call).\nX = df.drop(columns=[\"deposit\"])\ny = df[\"deposit\"]\n\n# Identify numerical columns for operations\nnumerical_cols = X.select_dtypes(include=np.number).columns\n\n# Replace infinite values in numerical columns of X with the median of each column\nfor col in numerical_cols:\n    # Using .loc to avoid FutureWarning with inplace=True on a copy\n    X.loc[:, col] = X[col].replace([float('inf'), float('-inf')], X[col].median())\n    # Fill NaN values in numerical columns with the median\n    X.loc[:, col] = X[col].fillna(X[col].median())\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.30, random_state=42, stratify=y\n)\n\nprint(\"Train shapes:\", X_train.shape, y_train.shape)\nprint(\"Test  shapes:\", X_test.shape, y_test.shape)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "76dd68c0-df62-4467-9b47-f550d689fde5",
      "cell_type": "code",
      "source": "from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\ndt = DecisionTreeClassifier(random_state=42)\n\n# Identify numerical and categorical columns\nnumerical_cols = X.select_dtypes(include=np.number).columns\ncategorical_cols = X.select_dtypes(include='object').columns\n\n# Create a preprocessor using ColumnTransformer\n# This will scale numerical features and one-hot encode categorical features\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numerical_cols),\n        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n    ])\n\n# Apply preprocessing to X_train and X_test\nX_train_processed = preprocessor.fit_transform(X_train)\nX_test_processed = preprocessor.transform(X_test)\n\n# Fit the Decision Tree model on the processed data\ndt.fit(X_train_processed, y_train)              # train\n\ny_pred_dt = dt.predict(X_test_processed)        # predict\n\nacc_dt = accuracy_score(y_test, y_pred_dt)\nprec_dt = precision_score(y_test, y_pred_dt, average=\"weighted\")\nrec_dt  = recall_score(y_test, y_pred_dt, average=\"weighted\")\nf1_dt   = f1_score(y_test, y_pred_dt, average=\"weighted\")\n\n(acc_dt, prec_dt, rec_dt, f1_dt)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "38e1fd42-9e61-477b-aa7e-7e9ecc53cdcc",
      "cell_type": "code",
      "source": "\n# Add SVM, KNN, RF metrics here following the Decision Tree pattern\n\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# SVM Classifier\nsvm = SVC(kernel=\"rbf\", random_state=42)\nsvm.fit(X_train_processed, y_train)\ny_pred_svm = svm.predict(X_test_processed)\nacc_svm = accuracy_score(y_test, y_pred_svm)\nprec_svm = precision_score(y_test, y_pred_svm, average=\"weighted\")\nrec_svm = recall_score(y_test, y_pred_svm, average=\"weighted\")\nf1_svm = f1_score(y_test, y_pred_svm, average=\"weighted\")\n\n# KNN Classifier\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train_processed, y_train)\ny_pred_knn = knn.predict(X_test_processed)\nacc_knn = accuracy_score(y_test, y_pred_knn)\nprec_knn = precision_score(y_test, y_pred_knn, average=\"weighted\")\nrec_knn = recall_score(y_test, y_pred_knn, average=\"weighted\")\nf1_knn = f1_score(y_test, y_pred_knn, average=\"weighted\")\n\n# Random Forest Classifier\nrf = RandomForestClassifier(n_estimators=200, random_state=42)\nrf.fit(X_train_processed, y_train)\ny_pred_rf = rf.predict(X_test_processed)\nacc_rf = accuracy_score(y_test, y_pred_rf)\nprec_rf = precision_score(y_test, y_pred_rf, average=\"weighted\")\nrec_rf = recall_score(y_test, y_pred_rf, average=\"weighted\")\nf1_rf = f1_score(y_test, y_pred_rf, average=\"weighted\")\n\nprint(\"SVM Metrics:\", (acc_svm, prec_svm, rec_svm, f1_svm))\nprint(\"KNN Metrics:\", (acc_knn, prec_knn, rec_knn, f1_knn))\nprint(\"Random Forest Metrics:\", (acc_rf, prec_rf, rec_rf, f1_rf))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ad572e72-931d-4316-b139-7fdb27d4ef72",
      "cell_type": "code",
      "source": "import pandas as pd\n\n# Assemble a table (start with Decision Tree row, then add others)\nresults = pd.DataFrame([\n    {\"Model\": \"Decision Tree\", \"Accuracy\": acc_dt, \"Precision(w)\": prec_dt, \"Recall(w)\": rec_dt, \"F1(w)\": f1_dt},\n    {\"Model\": \"SVM\", \"Accuracy\": acc_svm, \"Precision(w)\": prec_svm, \"Recall(w)\": rec_svm, \"F1(w)\": f1_svm},\n    {\"Model\": \"KNN\", \"Accuracy\": acc_knn, \"Precision(w)\": prec_knn, \"Recall(w)\": rec_knn, \"F1(w)\": f1_knn},\n    {\"Model\": \"Random Forest\", \"Accuracy\": acc_rf, \"Precision(w)\": prec_rf, \"Recall(w)\": rec_rf, \"F1(w)\": f1_rf}\n])\nresults",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9f7c705d-902d-4be1-92a3-c723aaa9e60b",
      "cell_type": "code",
      "source": "# Add rows for SVM, KNN, and Random Forest using your computed metrics.\n# Then sort by F1(w) descending.\n\n# results.sort_values(\"F1(w)\", ascending=False)\n\n# TODO: your code below\nresults.sort_values(\"F1(w)\", ascending=False)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f0b64e52-d500-4141-8b07-9c0133f56e47",
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\n\n# F1(w) bar chart\nax = results.plot(kind=\"bar\", x=\"Model\", y=\"F1(w)\", legend=False)\nplt.title(\"Model Comparison by F1 (weighted)\")\nplt.ylabel(\"F1 (weighted)\")\nplt.xlabel(\"Model\")\nplt.ylim(0, 1)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "bada5899-e7ad-4f3f-9188-00b12dbb0aab",
      "cell_type": "code",
      "source": "**Classification (Supervised Learning)**\n\n○      Apply at least three classifiers (e.g., Decision Tree, KNN, SVM, Random Forest, Logistic Regression).\n\n○      Use train/test split and evaluate with Accuracy, Precision, Recall, and F1 (weighted).\n\n○      The present results in a comparison table and plots.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b1ec5bcb-8008-4722-b00e-05d7b2f9406e",
      "cell_type": "code",
      "source": "# Import Libraries\n# import the necessary packages for data, preprocessing, and ML\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "43f306a1-5f9e-4037-84ce-2b37f7debe5e",
      "cell_type": "code",
      "source": "#  Load Data\n#  read the CSV file (Hint: pd.read_csv)\ndf = pd.read_csv(\"bank.csv\")\n#  Display first few rows\nprint(df.head())",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "fe666a16-8a36-49ad-9c92-0122fb7a1a46",
      "cell_type": "code",
      "source": "# Handle Missing Values\n# Check for missing values\nprint(df.isna().sum().sort_values(ascending=False))\n# Fill missing values with column mean (Hint: df.fillna(df.mean(numeric_only=True)))\ndf = df.fillna(df.mean(numeric_only=True))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8f62088f-0aa2-4a7e-920d-18d817d95f48",
      "cell_type": "code",
      "source": "# Separate Features and Target\n# Drop 'deposit' column from X and keep it as y\nX = df.drop(\"deposit\", axis=1)\ny = df[\"deposit\"]",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3a9d371c-8076-41e4-9488-14581fe1a47c",
      "cell_type": "code",
      "source": "# Preprocess and Scale Data\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\n# Identify numerical and categorical columns\nnumerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\ncategorical_cols = X.select_dtypes(include=['object']).columns\n\n# Create a preprocessor using ColumnTransformer\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numerical_cols),\n        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n    ])\n\n# Fit and transform the features\nX_scaled = preprocessor.fit_transform(X)\n\nprint(\"Data preprocessed and scaled successfully.\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "06831932-92bd-47e9-8cf6-532b4a003e76",
      "cell_type": "code",
      "source": "**Clustering (Unsupervised Learning)**\n\n○      Apply at least one clustering algorithm (e.g., K-means, hierarchical, DBSCAN).\n\n○      Evaluate results and provide visualizations\n\n○      Discuss whether clusters provide meaningful patterns.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b82a1145-14cc-4320-b397-9a573f533662",
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\n\nmobile = pd.read_csv('bank.csv')\nprint('Shape:', mobile.shape)\nmobile.info()\n\n# Check missing values\nmobile.isnull().sum()\n# Fill numeric columns with mean (safe even if none are missing)\nmobile.fillna(mobile.mean(numeric_only=True), inplace=True)\nprint('Total missing after fill:', mobile.isnull().sum().sum())",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "721132bb-aa6d-4684-baf4-d51a89c9b443",
      "cell_type": "code",
      "source": "from sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\n# Separate features from target for clustering\n# Assuming 'deposit' is the target column to be excluded from clustering features\nX_mobile = mobile.drop('deposit', axis=1)\n\n# Identify numerical and categorical columns in X_mobile\nnumerical_cols_mobile = X_mobile.select_dtypes(include=np.number).columns\ncategorical_cols_mobile = X_mobile.select_dtypes(include='object').columns\n\n# Create a preprocessor using ColumnTransformer\npreprocessor_mobile = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numerical_cols_mobile),\n        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols_mobile)\n    ])\n\n# Fit and transform the features\nX_scaled_for_clustering = preprocessor_mobile.fit_transform(X_mobile)\nprint('Scaled shape for clustering:', X_scaled_for_clustering.shape)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c9804f7f-5716-4acd-81b2-ff1ee5b774a1",
      "cell_type": "code",
      "source": "# Scale Data\n#  initialize a StandardScaler and fit_transform the features\nscaler = StandardScaler()\n# Select only numerical columns from X for StandardScaler\nnumerical_cols = X.select_dtypes(include=np.number).columns\nX_scaled = scaler.fit_transform(X[numerical_cols])",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8ad9f8aa-e848-41a8-93e5-7a7fe8ee7baa",
      "cell_type": "code",
      "source": "#  Apply K-Means\n# Try k=3 and set random_state=42 for reproducibility\nkmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\nclusters = kmeans.fit_predict(X_scaled_for_clustering)\nmobile[\"cluster_kmeans\"] = clusters\n# print cluster counts\nprint(mobile[\"cluster_kmeans\"].value_counts())",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "03f19f8b-a02c-414d-9fb1-38c7a5c85644",
      "cell_type": "code",
      "source": "from sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\nSSE = []\nfor k in range(1, 20):\n    km = KMeans(n_clusters=k, init='k-means++', random_state=42, n_init=10)\n    SSE.append(km.fit(X_scaled_for_clustering).inertia_)\n\nplt.plot(range(1, 20), SSE, marker='o')\nplt.title('Elbow Method (SSE vs k)')\nplt.xlabel('k')\nplt.ylabel('SSE')\nplt.show()\n\n#  Choose a reasonable k from the elbow. (Can you identify k with this SSE-based method?)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6ca77baf-0c3a-436f-8a54-fe284a3aae49",
      "cell_type": "code",
      "source": "from sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled_for_clustering)\nplt.scatter(X_pca[:,0], X_pca[:,1], c=mobile['cluster_kmeans'])\nplt.title('K-Means (PCA 2D)')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f841c0d9-648e-4abb-92ff-ab2b09a557c8",
      "cell_type": "code",
      "source": "# Choose two features (e.g., 'age' and 'balance')\nfeature_x = 'age'\nfeature_y = 'balance'\nplt.scatter(mobile[feature_x], mobile[feature_y], c=mobile['cluster_kmeans'])\nplt.xlabel(feature_x)\nplt.ylabel(feature_y)\nplt.title('K-Means by chosen features')\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1502fed2-2ed4-41b0-99a5-a614ae0eae00",
      "cell_type": "code",
      "source": "# Try another pair of features\nfeature_x = 'duration'\nfeature_y = 'campaign'\nplt.scatter(mobile[feature_x], mobile[feature_y], c=mobile['cluster_kmeans'])\nplt.xlabel(feature_x)\nplt.ylabel(feature_y)\nplt.title('K-Means by chosen features')\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "38ef1295-64a5-4391-9ff2-1d66a1a97854",
      "cell_type": "code",
      "source": "import scipy.cluster.hierarchy as sch\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(8,5))\ndendrogram = sch.dendrogram(sch.linkage(X_scaled_for_clustering, method='ward'))\nplt.title('Dendrogram')\nplt.xlabel('Sample index')\nplt.ylabel('Distance')\nplt.show()\n\n# Pick a reasonable k from the dendrogram\nk_dend = 4",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d4966061-5011-4136-9b50-725ec34343b2",
      "cell_type": "code",
      "source": "from sklearn.cluster import AgglomerativeClustering\n\nk_hier = 4\nhc = AgglomerativeClustering(n_clusters=k_hier, metric='euclidean', linkage='ward')\ny_hc = hc.fit_predict(X_scaled_for_clustering)\nmobile['cluster_hier'] = y_hc\n\n# Visualize in PCA space\nplt.scatter(X_pca[:,0], X_pca[:,1], c=y_hc)\nplt.title('Hierarchical (PCA 2D)')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3ed13add-3b58-414b-9059-2a845a2839c6",
      "cell_type": "code",
      "source": "#  Visualize Clusters\n# Reduce to 2D using PCA and plot\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled_for_clustering) # Use the scaled data for clustering\nplt.figure(figsize=(5,5))\nplt.scatter(X_pca[:,0], X_pca[:,1], c=mobile[\"cluster_kmeans\"], cmap='tab10', s=8) # Use mobile and cluster_kmeans\nplt.title(\"K-Means Clusters (k=3)\")\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "eb694daa-f61d-4d99-9ed4-0177d6ddbf56",
      "cell_type": "code",
      "source": "#  Train a Global Classifier\n# Split data, train DecisionTreeClassifier, and check accuracy\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\nmodel_global = DecisionTreeClassifier(random_state=42)\nmodel_global.fit(X_train, y_train)\ny_pred_global = model_global.predict(X_test)\nprint(\"Overall Accuracy:\", accuracy_score(y_test, y_pred_global))  # HINT: Expect around 0.8",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4f657d7c-65ee-4781-8075-12d4e3fafac0",
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Train Classifiers Per Cluster\ncluster_results = {}\n#  Loop through clusters and train separate models\nfor c in sorted(mobile[\"cluster_kmeans\"].unique()): # Use mobile and cluster_kmeans\n    subset = mobile[mobile[\"cluster_kmeans\"] == c].copy() # Use mobile and cluster_kmeans, and .copy() to avoid SettingWithCopyWarning\n\n    # Define features X_c and target y_c for the current subset\n    X_c = subset.drop(columns=[\"deposit\", \"cluster_kmeans\", \"cluster_hier\"]) # Drop deposit, cluster_kmeans, and cluster_hier\n    y_c = subset[\"deposit\"] # Use deposit as target\n\n    # Identify numerical and categorical columns for this subset\n    numerical_cols_c = X_c.select_dtypes(include=np.number).columns\n    categorical_cols_c = X_c.select_dtypes(include='object').columns\n\n    # Create a preprocessor for this subset\n    preprocessor_c = ColumnTransformer(\n        transformers=[\n            ('num', StandardScaler(), numerical_cols_c),\n            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols_c)\n        ],\n        remainder='passthrough' # Keep other columns if any, though likely none after selection\n    )\n\n    # Check if there's enough data for splitting and if y_c has at least two unique classes\n    if len(subset) < 2 or len(y_c.unique()) < 2:\n        print(f\"Cluster {c}: Not enough samples or classes to train a classifier. Skipping.\")\n        cluster_results[c] = np.nan # Assign NaN or other placeholder\n        continue\n\n    # Split the data for the current cluster\n    X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_c, y_c, test_size=0.3, random_state=42, stratify=y_c)\n\n    # Apply preprocessing to X_train and X_test for the current cluster\n    X_train_processed_c = preprocessor_c.fit_transform(X_train_c)\n    X_test_processed_c = preprocessor_c.transform(X_test_c)\n\n    model = DecisionTreeClassifier(random_state=42)\n    model.fit(X_train_processed_c, y_train_c) # Train on processed data\n    y_pred_c = model.predict(X_test_processed_c) # Predict on processed data\n    acc = accuracy_score(y_test_c, y_pred_c)\n    cluster_results[c] = acc\n    print(f\"Cluster {c}: Accuracy = {acc:.3f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b01bd2f5-e0cd-4fb6-99ef-1f9c67ee044a",
      "cell_type": "code",
      "source": "#  Visualize Cluster-Wise Accuracy\n#  Create a bar chart of cluster accuracies\nplt.bar(cluster_results.keys(), cluster_results.values(), color='lightgreen')\nplt.title(\"Classification Accuracy per Cluster (k=3)\")\nplt.xlabel(\"Cluster\")\nplt.ylabel(\"Accuracy\")\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "59128e99-a4df-47fe-9797-8cf314d91e45",
      "cell_type": "code",
      "source": "**Optional (Encouraged) Extensions**\n\n○      Outlier Detection (e.g., z-score, IQR, Isolation Forest).\n\n○      Association Rule Mining (e.g., Apriori, FP-Growth).\n\n○      Show how these methods could complement classification/clustering.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "bd33fd86-fb26-406e-9932-c1f7b567704f",
      "cell_type": "code",
      "source": "**Outlier Detection**",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "28844a6a-7dca-4cbe-bda6-14f6039ec1b0",
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\n# Load data\n# pd.read_csv('bank.csv')\ndf = pd.read_csv('bank.csv')\n\n# Based on the available data, 'price_range' is not present in 'bank.csv'.\n# Assuming we want to analyze numerical columns for outliers.\n# Let's select relevant numerical columns from bank.csv for outlier detection.\n# Exclude 'duration' and 'pdays' as they might have specific interpretations not ideal for generic outlier detection without more context.\nnumeric_cols = ['age', 'balance', 'campaign', 'previous']\n\n# Basic checks\n# Uncomment to inspect\ndisplay(df.head())\nprint(df[numeric_cols].isnull().sum())\n\n# Fill missing values so algorithms receive complete data\ndf = df.fillna(df.mean(numeric_only=True))\n\n# Standardize numeric features for LOF / KMeans\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(df[numeric_cols])\n\nprint(\"✅ Data loaded, cleaned (NaN filled), and standardized.\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7feb4527-2e6c-4962-9238-0b4ce735744f",
      "cell_type": "code",
      "source": "z_scores = np.abs((df[numeric_cols] - df[numeric_cols].mean()) / df[numeric_cols].std())\n\n#  Choose your threshold\nthreshold = 2.5\n\n# Flag a row as outlier if ANY feature exceeds the threshold\ndf['Outlier_Z'] = (z_scores > threshold).any(axis=1)\n\nprint(f\"[Z-Score] Detected {df['Outlier_Z'].sum()} outliers (threshold={threshold}).\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "615d9bcd-1973-4ecd-811e-58c2e09cd74d",
      "cell_type": "code",
      "source": "from sklearn.neighbors import LocalOutlierFactor\n\n# Fill parameters and experiment\nn_neighbors   = 40     # try 10, 20, 40\ncontamination = 0.10    # try 0.03, 0.05, 0.10\nmetric        = 'minkowski'\np_value       = 1    # 1 (Manhattan), 2 (Euclidean), 3 (higher-order)\n\nlof = LocalOutlierFactor(n_neighbors=n_neighbors,\n                         contamination=contamination,\n                         metric=metric,\n                         p=p_value)\n\n# Fit on standardized data and get predictions (1 = inlier, -1 = outlier)\ny_pred_lof = lof.fit_predict(scaled_data)\n\n# Persist LOF flags\ndf['Outlier_LOF'] = (y_pred_lof == -1)\n\nprint(f\"[LOF] Detected {df['Outlier_LOF'].sum()} outliers \"\n      f\"(n_neighbors={n_neighbors}, contamination={contamination}, p={p_value}).\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0cfcc4de-c6b9-4115-ab5c-3e44a6ebaac9",
      "cell_type": "code",
      "source": "from sklearn.cluster import KMeans\nfrom numpy.linalg import norm\nimport matplotlib.pyplot as plt\n\n# Choose k and distance cutoff percentile\nn_clusters = 5\nthreshold_percentile = 95\n\nkmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\nlabels = kmeans.fit_predict(scaled_data)\n\n# Distance to closest centroid (Euclidean norm)\ndistances = np.min(norm(scaled_data[:, np.newaxis] - kmeans.cluster_centers_, axis=2), axis=1)\n\n# Percentile-based cutoff -> boolean flag\ndist_thresh = np.percentile(distances, threshold_percentile)\ndf['Outlier_KMeans'] = (distances > dist_thresh)\n\nprint(f\"[K-Means] Detected {df['Outlier_KMeans'].sum()} outliers \"\n      f\"(k={n_clusters}, cutoff={threshold_percentile}th).\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a63f73e6-7de8-49fa-8ccc-61d53c59148f",
      "cell_type": "code",
      "source": "method_cols = ['Outlier_Z', 'Outlier_LOF', 'Outlier_KMeans']\n\n# Counts per method\ncounts = df[method_cols].sum().rename(\"Detected Outliers\")\nprint(\"Per‑method counts:\\n\", counts.to_frame())\n\n# Pairwise and triple overlaps\noverlap_Z_LOF  = (df['Outlier_Z'] & df['Outlier_LOF']).sum()\noverlap_Z_KM   = (df['Outlier_Z'] & df['Outlier_KMeans']).sum()\noverlap_LOF_KM = (df['Outlier_LOF'] & df['Outlier_KMeans']).sum()\noverlap_all3   = (df['Outlier_Z'] & df['Outlier_LOF'] & df['Outlier_KMeans']).sum()\n\nprint(\"\\nPairwise overlaps:\")\nprint(f\"Z ∩ LOF      : {overlap_Z_LOF}\")\nprint(f\"Z ∩ K-Means  : {overlap_Z_KM}\")\nprint(f\"LOF ∩ K-Means: {overlap_LOF_KM}\")\nprint(f\"All three    : {overlap_all3}\")\n\n# Consensus vote: how many methods (0..3) flag each row?\ndf['Outlier_Vote'] = df[method_cols].sum(axis=1)\ndf['Consensus_2plus'] = df['Outlier_Vote'] >= 2\nprint(f\"Consensus (≥2 methods agree): {df['Consensus_2plus'].sum()} rows\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f66b2d62-78d6-46b1-8435-dbf5127a9251",
      "cell_type": "code",
      "source": "**Association Rule Mining**",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "760a47a0-c802-45ee-a0b1-f04747901157",
      "cell_type": "code",
      "source": "#  Load the dataset using pandas\n#  use pd.read_csv('filename.csv')\nimport pandas as pd\n\ndf = pd.read_csv('bank.csv') \ndf.head()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "da854789-530b-4687-86a4-8a9bc8e79d0b",
      "cell_type": "code",
      "source": "import numpy as np\ncat_df = df.copy()\n\n# Example loop structure:\nfor col in df.columns:\n    if df[col].dtype != 'object' and col != 'deposit': # Exclude 'deposit' as it's already categorical\n        median_val = df[col].median()\n        new_col = col + '_cat'\n        cat_df[new_col] = ['low'if val <= median_val else 'high' for val in df[col]]\n\n# Add the 'deposit' column to cat_df as it's our target and already categorical\ncat_df['deposit'] = df['deposit']\n\ndisplay(cat_df.head()) # Display head to check the new DataFrame",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "86954f0d-df7f-411f-8622-9abaab1d5443",
      "cell_type": "code",
      "source": "# Keep only the categorical columns in cat_df\ncat_cols=[c for c in cat_df.columns if c.endswith('_cat')] + ['deposit']\ncat_df=cat_df[cat_cols]\ndisplay(cat_df)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6348b158-fd96-44da-a90b-787f86d5077c",
      "cell_type": "code",
      "source": "import pandas as pd\nfrom mlxtend.preprocessing import TransactionEncoder\nfrom mlxtend.frequent_patterns import apriori, association_rules\n\n# --- FIX: Load and prepare the data from 'bank.csv' ---\n# 1. Load the CSV file\ndf = pd.read_csv('bank.csv')\n\n# 2. Select the categorical columns for Apriori analysis\ncategorical_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome', 'deposit']\ncat_df = df[categorical_cols].copy()\n\n# 3. Convert all columns to string type\nfor col in cat_df.columns:\n    cat_df[col] = cat_df[col].astype(str)\n# --- END FIX ---\n\n# Prepare transactions\ntransactions = []\nfor i in range(len(cat_df)):\n    row = cat_df.iloc[i]\n    temp_list = []\n    for col in cat_df.columns:\n        # The structure is 'ColumnName=Value'\n        temp_list.append(col + '=' + str(row[col]))\n    transactions.append(temp_list)\n\n# Encode transactions for Apriori\nte = TransactionEncoder()\nonehot_df = te.fit_transform(transactions)\nonehot_df = pd.DataFrame(onehot_df, columns=te.columns_)\n\n# Run Apriori algorithm (min_support=0.2)\nfrequent_itemsets = apriori(onehot_df, min_support=0.2, use_colnames=True)\n\n# Generate and filter rules predicting deposit (min_threshold=0.7)\nrules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\nrules_target = rules[rules['consequents'].apply(lambda x: 'deposit=yes' in x)]\n\n# Show top 3 rules sorted by confidence\nshow_top_3 = rules_target.sort_values(by='confidence', ascending=False).head(3)\nprint(\"\\nTop 3 Association Rules Predicting 'deposit=yes':\")\nprint(show_top_3)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4b095c8d-c4ba-473c-8b5e-9cf2afaa8324",
      "cell_type": "code",
      "source": "#  Generate Association Rules\n\n# We will use the Apriori algorithm to find relationships (rules)\n# between feature combinations and the target variable 'price_range'.\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n#  Import required libraries ---\n#  Import TransactionEncoder, apriori, and association_rules from mlxtend\n# Hint: they come from mlxtend.frequent_patterns\nfrom mlxtend.preprocessing import TransactionEncoder\nfrom mlxtend.frequent_patterns import apriori, association_rules\n\n# Turn each row into a \"transaction\" list ---\n# Each transaction is a list like [\"ram_cat=high\", \"battery_power_cat=low\", ...]\ntransactions = []\nfor i in range(len(cat_df)):\n    row = cat_df.iloc[i]\n    temp_list = []\n    for col in cat_df.columns:\n        #  combine column name and value into one string\n        # Example: \"ram_cat=high\"\n        temp_list.append(col + \"=\" + str(row[col]))\n    transactions.append(temp_list)\n\n#  Encode transactions into one-hot format ---\n#  TransactionEncoder converts lists of strings into a DataFrame of 0/1 values.\nte = TransactionEncoder()\nte_data = te.fit(transactions).transform(transactions)\nonehot_df = pd.DataFrame(te_data, columns=te.columns_)\n\n# Run the Apriori algorithm ---\n# Try different min_support values (0.05 → 0.03) if you get no rules\n# max_len=3 means at most 2 items on left side + 1 item on right side\nfrequent_itemsets = apriori(\n    onehot_df,\n    min_support=0.05,\n    use_colnames=True,\n    max_len=3\n)\n\n# Generate rules from frequent itemsets ---\n# Generate rules using 'confidence' as the metric\nrules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.4)\n\n# Focus only on rules that predict price_range ---\n# filter rules where 'price_range' appears in the consequents (right side)\nrules_target = rules[rules[\"consequents\"].astype(str).str.contains(\"price_range\")]\n\n#  Keep rules with ≤2 features on the left side (antecedents) ---\nsimple_rules = []\nfor i in range(len(rules_target)):\n    if len(rules_target.iloc[i][\"antecedents\"]) <= 2:\n        simple_rules.append(rules_target.iloc[i])\n\n# Sort and show the top 3 rules ---\nif len(simple_rules) == 0:\n    print(\"⚠️ No rules found. Try lowering min_support or min_threshold.\")\nelse:\n    simple_rules_df = pd.DataFrame(simple_rules)\n    top3 = simple_rules_df.sort_values(\"confidence\", ascending=False).head(3)\n    print(\"Top 3 rules by confidence:\")\n    display(top3[[\"antecedents\", \"consequents\", \"support\", \"confidence\", \"lift\"]])\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c34801e4-f731-4d58-b87f-cbab165ecea0",
      "cell_type": "code",
      "source": "#  Add rule-based features manually based on your top 3 rules\n# Example:\n\n# Reconstruct cat_df as it was intended after cell 5Utt6aPgNKnj,\n# which had binned numerical columns and the 'deposit' column.\n# This is necessary because cell 0tU3F7EqKP0W overwrote `cat_df` with only original categorical columns.\n\n# Assuming 'df' (the original bank.csv dataframe) is available from previous cells.\ntemp_cat_df = df.copy()\n\n# Apply the same binning logic as in cell dMDHHFkPNH4S\nfor col in df.columns:\n    if df[col].dtype != 'object' and col != 'deposit':\n        median_val = df[col].median()\n        new_col = col + '_cat'\n        temp_cat_df[new_col] = ['low' if val <= median_val else 'high' for val in df[col]]\n\n# Add the 'deposit' column to temp_cat_df as it's our target and already categorical\ntemp_cat_df['deposit'] = df['deposit']\n\n# Filter temp_cat_df to only keep the _cat columns and 'deposit' as in cell 5Utt6aPgNKnj\ncat_cols_filtered = [c for c in temp_cat_df.columns if c.endswith('_cat')] + ['deposit']\ncat_df = temp_cat_df[cat_cols_filtered].copy() # Assign to cat_df, as expected by the rest of the cell\n\nenhanced_df = cat_df.copy()\n\n# Create example rule-based features using existing columns in cat_df\n# These rules are illustrative since no rules were generated in the previous step with the given thresholds.\n\n# Rule 1: High age AND High balance\nrule1 = []\nfor i in range(len(cat_df)):\n    if cat_df.iloc[i]['age_cat'] == 'high' and cat_df.iloc[i]['balance_cat'] == 'high':\n        rule1.append(1)\n    else:\n        rule1.append(0)\nenhanced_df['Rule1'] = rule1\n\n# Rule 2: High duration AND Deposit is 'yes'\nrule2 = []\nfor i in range(len(cat_df)):\n    if cat_df.iloc[i]['duration_cat'] == 'high' and cat_df.iloc[i]['deposit'] == 'yes':\n        rule2.append(1)\n    else:\n        rule2.append(0)\nenhanced_df['Rule2'] = rule2\n\n# Rule 3: Low age AND Low campaign\nrule3 = []\nfor i in range(len(cat_df)):\n    if cat_df.iloc[i]['age_cat'] == 'low' and cat_df.iloc[i]['campaign_cat'] == 'low':\n        rule3.append(1)\n    else:\n        rule3.append(0)\nenhanced_df['Rule3'] = rule3",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "bed750ee-3226-42f2-a244-f722b25c3130",
      "cell_type": "code",
      "source": "ench_df = enhanced_df.copy()\nench_df.head()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "02c345c9-a24e-4432-bfa0-08f8c64042f0",
      "cell_type": "code",
      "source": "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nimport pandas as pd\nimport numpy as np\n\n# Prepare data\nX_base = df.drop(columns=['deposit'])\ny = df['deposit']\n\n# Identify numerical and categorical columns for X_base\nnumerical_cols_base = X_base.select_dtypes(include=np.number).columns\ncategorical_cols_base = X_base.select_dtypes(include='object').columns\n\n# Create a preprocessor for the baseline model\npreprocessor_base = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numerical_cols_base),\n        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols_base)\n    ])\n\n# Apply preprocessing to X_base\nX_base_processed = preprocessor_base.fit_transform(X_base)\n\n# One-hot encode X_enhanced for model training (this part was already mostly correct)\n# First, identify the categorical columns in enhanced_df (excluding 'deposit')\ncategorical_cols_enhanced = [col for col in enhanced_df.columns if (col.endswith('_cat') or col.startswith('Rule')) and col != 'deposit']\n\n# Initialize OneHotEncoder for enhanced features\nencoder_enhanced = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n\n# Fit and transform the categorical columns\nX_enhanced_encoded = encoder_enhanced.fit_transform(enhanced_df[categorical_cols_enhanced])\n\n# Create a DataFrame for the encoded enhanced features\nX_enhanced_processed = pd.DataFrame(X_enhanced_encoded, columns=encoder_enhanced.get_feature_names_out(categorical_cols_enhanced))\n\n# Ensure y matches the index for train_test_split for enhanced model\ny_processed = enhanced_df['deposit'].reset_index(drop=True)\n\n\n# Split data for baseline model (using X_base_processed)\nX_train_base, X_test_base, y_train_base, y_test_base = train_test_split(X_base_processed, y, test_size=0.2, random_state=42)\n\n# Train baseline Random Forest\nrf1 = RandomForestClassifier(random_state=42)\nrf1.fit(X_train_base, y_train_base)\npred_base = rf1.predict(X_test_base)\n\n# Split data for enhanced model\nX_train_enhanced, X_test_enhanced, y_train_enhanced, y_test_enhanced = train_test_split(X_enhanced_processed, y_processed, test_size=0.2, random_state=42)\n\n# Train enhanced Random Forest\nrf2 = RandomForestClassifier(random_state=42)\nrf2.fit(X_train_enhanced, y_train_enhanced)\npred_enhanced = rf2.predict(X_test_enhanced)\n\n# Compute F1-scores\nf1_base = f1_score(y_test_base, pred_base, average='weighted') # Use 'weighted' for multi-class classification\nf1_enhanced = f1_score(y_test_enhanced, pred_enhanced, average='weighted') # Use 'weighted' for multi-class classification\n\n# Print results\nprint(f'Baseline F1-score: {f1_base:.4f}')\nprint(f'Enhanced F1-score: {f1_enhanced:.4f}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6e30b31c-efd6-4f9c-a234-c418a724dff2",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}